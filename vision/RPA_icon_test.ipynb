{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/c/Users/weiwya/Desktop/icon_data_encoded/'\n",
    "files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(root):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, tests_data = {}, {}\n",
    "for f in files:\n",
    "    try:\n",
    "        d = pickle.load(open(\"%s%s\" %(root, f), 'rb' ))\n",
    "        k = f[:-2]\n",
    "        train, test = train_test_split(d, test_size=0.2, random_state=1234)\n",
    "        train_data[k] = train\n",
    "        tests_data[k] = test\n",
    "    except:\n",
    "        print(f)\n",
    "#     print(k, len(train), len(test), len(d))\n",
    "neg_train = train_data['_negative']\n",
    "neg_test = tests_data['_negative']\n",
    "del train_data['_negative']\n",
    "del tests_data['_negative']\n",
    "class_names = np.array(list(train_data.keys()))\n",
    "class_name_to_idx = {n:i for i, n in enumerate(class_names)}\n",
    "idx_to_class_name = {v:k for k, v in class_name_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dict, classes, idx_to_class_name, return_size=None, neg_samples=None):\n",
    "    data = []\n",
    "    labels = []\n",
    "    counts = 0\n",
    "\n",
    "    for c in classes:\n",
    "        class_name = idx_to_class_name[c]\n",
    "        d = data_dict[class_name]\n",
    "        size = d.shape[0]\n",
    "        if return_size is None or size<return_size:\n",
    "            data.append(d)\n",
    "            counts += size\n",
    "            labels += [c] * size\n",
    "        else:\n",
    "            data.append(d[:return_size])\n",
    "            counts += return_size\n",
    "            labels += [c]* return_size  \n",
    "    \n",
    "    \n",
    "    if neg_samples is not None:\n",
    "        idx = np.random.choice(neg_samples.shape[0], neg_samples.shape[0],replace=False)\n",
    "        if return_size is not None:\n",
    "            data.append(neg_samples[idx[:return_size]])\n",
    "            labels+= [-1] * return_size\n",
    "            counts += return_size\n",
    "\n",
    "        else:\n",
    "            data.append(neg_samples[idx])\n",
    "            labels += [-1] * neg_samples.shape[0]\n",
    "            counts += neg_samples.shape[0]\n",
    "            \n",
    "        #add other classes as neg examples as well\n",
    "        other = []\n",
    "        for k, v in data_dict.items():\n",
    "            if k not in classes:\n",
    "                other.append(v)\n",
    "        other = np.vstack(other)\n",
    "        sample_idx = np.random.choice(other.shape[0],  other.shape[0],  replace=False)\n",
    "        if return_size is not None:\n",
    "            other = other[sample_idx[:return_size]]\n",
    "            labels += [-1] * return_size\n",
    "            counts += return_size\n",
    "        else:\n",
    "            #TODO!!!! set better size here\n",
    "            other = other[sample_idx[:1000]]\n",
    "            labels += [-1] * 1000\n",
    "            counts += 1000\n",
    "            \n",
    "        data.append(other)\n",
    "    \n",
    "            \n",
    "    #shuffle data for good measure\n",
    "    data = np.vstack(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    idx = np.random.choice(np.arange(counts), size=counts, replace=False)\n",
    "    return data[idx], np.array(labels)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic MLP \n",
    "#TODO:Swap out for more  sophisticated models \n",
    "\n",
    "def fit_basic_model(train_data, train_labels, epochs=50, verbose=False):\n",
    "    input_dim = train_data.shape[1]\n",
    "    n_classes = len(np.unique(train_labels))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(input_dim, )))\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, to_categorical(train_labels), \n",
    "                        epochs = epochs, \n",
    "                        verbose = verbose,\n",
    "                        validation_split = 0.25,\n",
    "                        shuffle=True)\n",
    "    return model\n",
    "\n",
    "    \n",
    "def get_accuracy(predict, acutual):\n",
    "    return np.sum(predict==acutual)/len(acutual)\n",
    "\n",
    "\n",
    "def gen_task_classes(total_classes, n_tasks=10, shuffle=False):\n",
    "    class_labels = np.arange(total_classes)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(class_labels)\n",
    "    return np.array_split(class_labels, n_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_all_task_models(train_dict, idx_to_class_name, neg_samples, n_tasks=10, n_sample_per_task=1000, \n",
    "                        n_label_repeats=1, shuffle_labels=True,  verbose=False):\n",
    "    \n",
    "    total_labels = len(train_dict)\n",
    "    print('total_labels %s' %total_labels)\n",
    "    print('n_tasks!!! %s' %n_tasks)\n",
    "\n",
    "    labels_for_tasks = []\n",
    "    for i in range(n_label_repeats):\n",
    "        labels_for_tasks.append(gen_task_classes(total_labels, n_tasks, shuffle=shuffle_labels))\n",
    "\n",
    "\n",
    "    labels_for_tasks = np.vstack(labels_for_tasks)\n",
    "    total_tasks = len(labels_for_tasks)\n",
    "    print(\"total number of tasks %s\" %total_tasks)\n",
    "                         \n",
    "    transformers = {}\n",
    "    task_data = {}\n",
    "    task_label_lookup = {}\n",
    "    voters = {}\n",
    "    #this part tranins transformers for all tasks\n",
    "    for task in range(total_tasks):\n",
    "        actual_label_to_task_labels = {}\n",
    "        task_labels_to_actual_labels = {} \n",
    "        \n",
    "        task_labels = labels_for_tasks[task]        \n",
    "        data, label = get_data(train_dict, task_labels, idx_to_class_name, return_size=n_sample_per_task,\n",
    "                               neg_samples=neg_samples)\n",
    "        \n",
    "        task_labels = np.append(task_labels, -1) \n",
    "        task_labels = np.sort(task_labels) \n",
    "        \n",
    "        for i, l in enumerate(task_labels):\n",
    "            actual_label_to_task_labels[l] = i\n",
    "            task_labels_to_actual_labels[i] = l\n",
    "\n",
    "        label = [actual_label_to_task_labels[x] for x in label]\n",
    "        \n",
    "        model = fit_basic_model(data, label, verbose=verbose)\n",
    "       \n",
    "        #store for later stage\n",
    "        transformers[task] = model\n",
    "        task_label_lookup[task] = task_labels_to_actual_labels\n",
    "        task_data[task] = (data,label)\n",
    "                \n",
    "        print('done_transformer %s' %task)\n",
    "    print('training deciders')\n",
    "    \n",
    "    #this part trains voter, using transformers previously trained\n",
    "    for task in range(total_tasks):\n",
    "        d, l = task_data[task]\n",
    "        transformed = []\n",
    "        #partition current task data using all  models \n",
    "        for m in transformers.values():\n",
    "            mm = Model(inputs=m.inputs, outputs=m.layers[-2].output)\n",
    "            transformed.append(mm.predict(d))\n",
    "        transformed = np.hstack(transformed)\n",
    "        voter = fit_basic_model(transformed, l, verbose=verbose)\n",
    "        voters[task] = voter\n",
    "        print('done_voter %s' %task)\n",
    "    \n",
    "    return transformers, voters, task_label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the decider part of task-unaware, \n",
    "#currenly it does argmax of sums\n",
    "#TODO: exp w argmax of argmax, \n",
    "def vote_on_data(data, transforms, voters, task_label_lookup, total_classes=105):\n",
    "    n_tasks = len(voters)\n",
    "    n_classes = total_classes\n",
    "    n_data = len(data)\n",
    "\n",
    "    transformed = []\n",
    "    for n in range(n_tasks):\n",
    "        t = transformers[n]\n",
    "        m = Model(inputs=t.inputs, outputs=t.layers[-2].output)\n",
    "        transformed.append(m.predict(data))\n",
    "    transformed  = np.hstack(transformed)\n",
    "\n",
    "\n",
    "    all_prob = np.zeros((n_data, total_classes+1))\n",
    "\n",
    "    for n in range(n_tasks):\n",
    "        label = task_label_lookup[n]\n",
    "        v = voters[n]\n",
    "        vv = v.predict(transformed)\n",
    "        for i, p in enumerate(vv):\n",
    "            for j, pp in enumerate(p):\n",
    "                all_prob[i][label[j]] +=  pp\n",
    "    \n",
    "    all_prob = all_prob[:, :-1]\n",
    "    return all_prob.argmax(axis=1)\n",
    "\n",
    "def get_task_prediction(data, transformers, voters, task, task_label_lookup):\n",
    "    t = transformers[task]\n",
    "    prd = t.predict(data).argmax(axis=1) \n",
    "    actual = []\n",
    "    for p in prd:\n",
    "        actual.append(task_label_lookup[task][p])\n",
    "    return np.array(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_labels 105\n",
      "n_tasks!!! 7\n",
      "total number of tasks 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiwya/essex-graphs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:35: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done_transformer 0\n",
      "done_transformer 1\n",
      "done_transformer 2\n",
      "done_transformer 3\n",
      "done_transformer 4\n",
      "done_transformer 5\n",
      "done_transformer 6\n",
      "done_transformer 7\n",
      "done_transformer 8\n",
      "done_transformer 9\n",
      "done_transformer 10\n",
      "done_transformer 11\n",
      "done_transformer 12\n",
      "done_transformer 13\n",
      "done_transformer 14\n",
      "done_transformer 15\n",
      "done_transformer 16\n",
      "done_transformer 17\n",
      "done_transformer 18\n",
      "done_transformer 19\n",
      "done_transformer 20\n",
      "done_transformer 21\n",
      "done_transformer 22\n",
      "done_transformer 23\n",
      "done_transformer 24\n",
      "done_transformer 25\n",
      "done_transformer 26\n",
      "done_transformer 27\n",
      "done_transformer 28\n",
      "done_transformer 29\n",
      "done_transformer 30\n",
      "done_transformer 31\n",
      "done_transformer 32\n",
      "done_transformer 33\n",
      "done_transformer 34\n",
      "done_transformer 35\n",
      "done_transformer 36\n",
      "done_transformer 37\n",
      "done_transformer 38\n",
      "done_transformer 39\n",
      "done_transformer 40\n",
      "done_transformer 41\n",
      "done_transformer 42\n",
      "done_transformer 43\n",
      "done_transformer 44\n",
      "done_transformer 45\n",
      "done_transformer 46\n",
      "done_transformer 47\n",
      "done_transformer 48\n",
      "done_transformer 49\n",
      "done_transformer 50\n",
      "done_transformer 51\n",
      "done_transformer 52\n",
      "done_transformer 53\n",
      "done_transformer 54\n",
      "done_transformer 55\n",
      "done_transformer 56\n",
      "done_transformer 57\n",
      "done_transformer 58\n",
      "done_transformer 59\n",
      "done_transformer 60\n",
      "done_transformer 61\n",
      "done_transformer 62\n",
      "done_transformer 63\n",
      "done_transformer 64\n",
      "done_transformer 65\n",
      "done_transformer 66\n",
      "done_transformer 67\n",
      "done_transformer 68\n",
      "done_transformer 69\n",
      "training deciders\n",
      "done_voter 0\n",
      "done_voter 1\n",
      "done_voter 2\n",
      "done_voter 3\n",
      "done_voter 4\n",
      "done_voter 5\n",
      "done_voter 6\n",
      "done_voter 7\n",
      "done_voter 8\n",
      "done_voter 9\n",
      "done_voter 10\n",
      "done_voter 11\n",
      "done_voter 12\n",
      "done_voter 13\n",
      "done_voter 14\n",
      "done_voter 15\n",
      "done_voter 16\n",
      "done_voter 17\n",
      "done_voter 18\n",
      "done_voter 19\n",
      "done_voter 20\n",
      "done_voter 21\n",
      "done_voter 22\n",
      "done_voter 23\n",
      "done_voter 24\n",
      "done_voter 25\n",
      "done_voter 26\n",
      "done_voter 27\n",
      "done_voter 28\n",
      "done_voter 29\n",
      "done_voter 30\n",
      "done_voter 31\n",
      "done_voter 32\n",
      "done_voter 33\n",
      "done_voter 34\n",
      "done_voter 35\n",
      "done_voter 36\n",
      "done_voter 37\n",
      "done_voter 38\n",
      "done_voter 39\n",
      "done_voter 40\n",
      "done_voter 41\n",
      "done_voter 42\n",
      "done_voter 43\n",
      "done_voter 44\n",
      "done_voter 45\n",
      "done_voter 46\n",
      "done_voter 47\n",
      "done_voter 48\n",
      "done_voter 49\n",
      "done_voter 50\n",
      "done_voter 51\n",
      "done_voter 52\n",
      "done_voter 53\n",
      "done_voter 54\n",
      "done_voter 55\n",
      "done_voter 56\n",
      "done_voter 57\n",
      "done_voter 58\n",
      "done_voter 59\n",
      "done_voter 60\n",
      "done_voter 61\n",
      "done_voter 62\n",
      "done_voter 63\n",
      "done_voter 64\n",
      "done_voter 65\n",
      "done_voter 66\n",
      "done_voter 67\n",
      "done_voter 68\n",
      "done_voter 69\n"
     ]
    }
   ],
   "source": [
    "transformers, voters, task_label_lookup = \\\n",
    "        fit_all_task_models(train_data, idx_to_class_name, n_tasks=7, \n",
    "                            n_label_repeats=10, n_sample_per_task=1000, neg_samples=neg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 10 0.5988372093023255\n",
      "airplane 10 0.609375\n",
      "alarm 10 0.23711340206185566\n",
      "arrow_down 10 0.43155452436194897\n",
      "arrow_left 10 0.22151898734177214\n",
      "arrow_right 10 0.18012422360248448\n",
      "arrow_up 10 0.5397727272727273\n",
      "attach 10 0.0\n",
      "bag 10 0.1588785046728972\n",
      "barcode 10 0.7171717171717171\n",
      "battery 10 0.435\n",
      "bluetooth 10 0.6125461254612546\n",
      "bookmark 10 0.5910543130990416\n",
      "brightness 10 0.40336134453781514\n",
      "calculator 10 0.652452025586354\n",
      "calendar 10 0.528\n",
      "call 10 0.5024630541871922\n",
      "camera 10 0.7029096477794793\n",
      "car 10 0.6612111292962357\n",
      "cart 10 0.6723484848484849\n",
      "chart 10 0.28346456692913385\n",
      "check_mark 10 0.603448275862069\n",
      "clock 10 0.6303501945525292\n",
      "close 10 0.5798319327731093\n",
      "cloud 10 0.6882591093117408\n",
      "computer 10 0.4691358024691358\n",
      "contrast 10 0.453125\n",
      "credit_card 10 0.559610705596107\n",
      "crop 10 0.44360902255639095\n",
      "cursor 10 0.43333333333333335\n",
      "cut 10 0.6336898395721925\n",
      "dashboard 10 0.49074074074074076\n",
      "delete 10 0.23404255319148937\n",
      "dollar 10 0.47468354430379744\n",
      "download 10 0.09574468085106383\n",
      "edit 10 0.6138613861386139\n",
      "external_link 10 0.11320754716981132\n",
      "eye 10 0.7353951890034365\n",
      "fab 10 0.7560975609756098\n",
      "facebook 10 0.6041666666666666\n",
      "fast_forward 10 0.23404255319148937\n",
      "favorite 10 0.5321100917431193\n",
      "file 10 0.5534883720930233\n",
      "filter 10 0.6153846153846154\n",
      "fingerprint 10 0.5909090909090909\n",
      "fire 10 0.5161290322580645\n",
      "flag 10 0.59375\n",
      "flashlight 10 0.2028985507246377\n",
      "folder 10 0.5219072164948454\n",
      "gift 10 0.6093023255813953\n",
      "globe 10 0.7175\n",
      "gmail 10 0.0\n",
      "google 10 0.38095238095238093\n",
      "grid 10 0.5570469798657718\n",
      "headphones 10 0.7180232558139535\n",
      "home 10 0.7017828200972447\n",
      "inbox 10 0.0\n",
      "info 10 0.4696969696969697\n",
      "laptop 10 0.5711009174311926\n",
      "light_bulb 10 0.5929203539823009\n",
      "link 10 0.5863636363636363\n",
      "location 10 0.5691964285714286\n",
      "lock 10 0.6143250688705234\n",
      "mail 10 0.6637168141592921\n",
      "map 10 0.5804878048780487\n",
      "maximize 10 0.2536231884057971\n",
      "megaphone 10 0.5780730897009967\n",
      "menu 10 0.6264591439688716\n",
      "microphone 10 0.5661764705882353\n",
      "minimize 10 0.4226190476190476\n",
      "mobile 10 0.5846153846153846\n",
      "moon 10 0.6528662420382165\n",
      "music 10 0.5257731958762887\n",
      "mute 10 0.2663316582914573\n",
      "notifications 10 0.5641025641025641\n",
      "overflow_menu 10 0.3333333333333333\n",
      "pinterest 10 0.0\n",
      "play 10 0.5272727272727272\n",
      "printer 10 0.6492890995260664\n",
      "profile_avatar 10 0.5454545454545454\n",
      "qr_code 10 0.6993865030674846\n",
      "question 10 0.5588235294117647\n",
      "refresh 10 0.5122950819672131\n",
      "reply 10 0.3973509933774834\n",
      "rewind 10 0.4742268041237113\n",
      "save 10 0.46683673469387754\n",
      "search 10 0.6984615384615385\n",
      "send 10 0.6108949416342413\n",
      "settings 10 0.5737051792828686\n",
      "share 10 0.3608815426997245\n",
      "signal 10 0.511049723756906\n",
      "sort 10 0.39436619718309857\n",
      "tag 10 0.5753012048192772\n",
      "television 10 0.5620689655172414\n",
      "thumbs_up 10 0.6785714285714286\n",
      "ticket 10 0.46407185628742514\n",
      "trash 10 0.5539568345323741\n",
      "trophy 10 0.7142857142857143\n",
      "twitter 10 0.1111111111111111\n",
      "unlock 10 0.4669603524229075\n",
      "upload 10 0.08333333333333333\n",
      "user 10 0.43610547667342797\n",
      "video_camera 10 0.47005988023952094\n",
      "volume 10 0.5506329113924051\n",
      "warning 10 0.575\n",
      "0.5469705974897738\n"
     ]
    }
   ],
   "source": [
    "#check how many times an label occurred in all tasks\n",
    "from collections import defaultdict\n",
    "occur = defaultdict(int)\n",
    "for v in task_label_lookup.values():\n",
    "    for vv in v.values():\n",
    "        occur[vv] += 1\n",
    "        \n",
    "total_correct = 0\n",
    "total = 0\n",
    "total_classes = len(train_data)\n",
    "for label, data in tests_data.items(): \n",
    "    predict = vote_on_data(data, transformers, voters, task_label_lookup,total_classes=total_classes)\n",
    "    c_class = class_name_to_idx[label]\n",
    "    total_correct += np.sum( predict == [c_class]*len(data)) \n",
    "    total += data.shape[0]\n",
    "    print(label,  occur[c_class], get_accuracy(predict, [c_class]*len(data)))\n",
    "\n",
    "print(total_correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 10 0.6773255813953488\n",
      "airplane 10 0.783984375\n",
      "alarm 10 0.5407216494845362\n",
      "arrow_down 10 0.6761020881670533\n",
      "arrow_left 10 0.6775316455696202\n",
      "arrow_right 10 0.49316770186335407\n",
      "arrow_up 10 0.6778409090909091\n",
      "attach 10 0.0\n",
      "bag 10 0.33551401869158876\n",
      "barcode 10 0.7747474747474747\n",
      "battery 10 0.6502500000000001\n",
      "bluetooth 10 0.7468634686346863\n",
      "bookmark 10 0.7335463258785943\n",
      "brightness 10 0.5109243697478991\n",
      "calculator 10 0.8439232409381663\n",
      "calendar 10 0.7847999999999999\n",
      "call 10 0.6709359605911329\n",
      "camera 10 0.7946401225114854\n",
      "car 10 0.7944353518821603\n",
      "cart 10 0.7886363636363636\n",
      "chart 10 0.5167322834645669\n",
      "check_mark 10 0.725\n",
      "clock 10 0.8182879377431908\n",
      "close 10 0.7468487394957983\n",
      "cloud 10 0.8155870445344129\n",
      "computer 10 0.6920987654320988\n",
      "contrast 10 0.56875\n",
      "credit_card 10 0.7197080291970803\n",
      "crop 10 0.5894736842105264\n",
      "cursor 10 0.6693333333333333\n",
      "cut 10 0.7970588235294118\n",
      "dashboard 10 0.6893518518518519\n",
      "delete 10 0.5340425531914893\n",
      "dollar 10 0.6645569620253164\n",
      "download 10 0.33936170212765954\n",
      "edit 10 0.7103960396039604\n",
      "external_link 10 0.20754716981132076\n",
      "eye 10 0.8378006872852234\n",
      "fab 10 0.8292682926829268\n",
      "facebook 10 0.6791666666666667\n",
      "fast_forward 10 0.5680851063829787\n",
      "favorite 10 0.684862385321101\n",
      "file 10 0.7065116279069767\n",
      "filter 10 0.7619230769230769\n",
      "fingerprint 10 0.7348484848484849\n",
      "fire 10 0.6870967741935484\n",
      "flag 10 0.6970833333333334\n",
      "flashlight 10 0.408695652173913\n",
      "folder 10 0.6708762886597939\n",
      "gift 10 0.7423255813953488\n",
      "globe 10 0.8079999999999999\n",
      "gmail 10 0.0\n",
      "google 10 0.40476190476190477\n",
      "grid 10 0.8161073825503355\n",
      "headphones 10 0.8203488372093023\n",
      "home 10 0.7878444084278768\n",
      "inbox 10 0.025\n",
      "info 10 0.5803030303030303\n",
      "laptop 10 0.7564220183486239\n",
      "light_bulb 10 0.7254424778761062\n",
      "link 10 0.6763636363636364\n",
      "location 10 0.6595982142857143\n",
      "lock 10 0.7672176308539945\n",
      "mail 10 0.796755162241888\n",
      "map 10 0.7563414634146342\n",
      "maximize 10 0.5369565217391304\n",
      "megaphone 10 0.7401993355481727\n",
      "menu 10 0.777431906614786\n",
      "microphone 10 0.6867647058823529\n",
      "minimize 10 0.6178571428571429\n",
      "mobile 10 0.7261538461538461\n",
      "moon 10 0.8363057324840765\n",
      "music 10 0.6518041237113402\n",
      "mute 10 0.5758793969849246\n",
      "notifications 10 0.7054487179487179\n",
      "overflow_menu 10 0.48461538461538456\n",
      "pinterest 10 0.0\n",
      "play 10 0.6368181818181818\n",
      "printer 10 0.8170616113744076\n",
      "profile_avatar 10 0.4681818181818182\n",
      "qr_code 10 0.8208588957055215\n",
      "question 10 0.7069852941176471\n",
      "refresh 10 0.6532786885245901\n",
      "reply 10 0.48079470198675495\n",
      "rewind 10 0.7355670103092783\n",
      "save 10 0.6852040816326531\n",
      "search 10 0.7624615384615385\n",
      "send 10 0.7478599221789883\n",
      "settings 10 0.703386454183267\n",
      "share 10 0.5705234159779614\n",
      "signal 10 0.6613259668508288\n",
      "sort 10 0.5873239436619718\n",
      "tag 10 0.6885542168674699\n",
      "television 10 0.7524137931034482\n",
      "thumbs_up 10 0.7892857142857143\n",
      "ticket 10 0.6688622754491018\n",
      "trash 10 0.7456834532374101\n",
      "trophy 10 0.8119047619047619\n",
      "twitter 10 0.25555555555555554\n",
      "unlock 10 0.7436123348017621\n",
      "upload 10 0.27708333333333335\n",
      "user 10 0.6551724137931034\n",
      "video_camera 10 0.6991017964071856\n",
      "volume 10 0.6672995780590718\n",
      "warning 10 0.745625\n",
      "0.7089151487606195\n"
     ]
    }
   ],
   "source": [
    "class_to_task_look_up = defaultdict(list)\n",
    "for task, v in task_label_lookup.items():\n",
    "    for vv in v.values():\n",
    "        class_to_task_look_up[vv].append(task)\n",
    "\n",
    "total_classes = len(train_data)\n",
    "total_correct, total =0,0\n",
    "for label, data in tests_data.items(): \n",
    "    c_class = class_name_to_idx[label]\n",
    "    tasks = class_to_task_look_up[c_class]\n",
    "    correct = 0\n",
    "    for task in tasks:\n",
    "        #TODO, need to average prob\n",
    "        predict = get_task_prediction(data, transformers, voters, task, task_label_lookup)\n",
    "        correct += np.sum( predict == [c_class]*len(data))\n",
    "    correct /= len(tasks)\n",
    "    total_correct += correct\n",
    "    \n",
    "    total += data.shape[0]\n",
    "    print(label,  occur[c_class], correct/data.shape[0])\n",
    "\n",
    "print(total_correct / total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81261, 1000) (81261,)\n",
      "Epoch 1/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 3.9086 - accuracy: 0.1385 - val_loss: 3.4878 - val_accuracy: 0.2123\n",
      "Epoch 2/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 3.3559 - accuracy: 0.2307 - val_loss: 3.2200 - val_accuracy: 0.2655\n",
      "Epoch 3/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 3.1543 - accuracy: 0.2662 - val_loss: 3.0730 - val_accuracy: 0.2941\n",
      "Epoch 4/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 3.0191 - accuracy: 0.2938 - val_loss: 2.9699 - val_accuracy: 0.3106\n",
      "Epoch 5/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.9195 - accuracy: 0.3126 - val_loss: 2.8874 - val_accuracy: 0.3330\n",
      "Epoch 6/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.8371 - accuracy: 0.3313 - val_loss: 2.8265 - val_accuracy: 0.3468\n",
      "Epoch 7/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.7670 - accuracy: 0.3462 - val_loss: 2.7680 - val_accuracy: 0.3602\n",
      "Epoch 8/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.7073 - accuracy: 0.3580 - val_loss: 2.7226 - val_accuracy: 0.3705\n",
      "Epoch 9/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.6559 - accuracy: 0.3689 - val_loss: 2.6855 - val_accuracy: 0.3776\n",
      "Epoch 10/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.6055 - accuracy: 0.3796 - val_loss: 2.6488 - val_accuracy: 0.3910\n",
      "Epoch 11/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.5595 - accuracy: 0.3881 - val_loss: 2.6177 - val_accuracy: 0.3943\n",
      "Epoch 12/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.5232 - accuracy: 0.3958 - val_loss: 2.5878 - val_accuracy: 0.4052\n",
      "Epoch 13/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.4879 - accuracy: 0.4041 - val_loss: 2.5635 - val_accuracy: 0.4087\n",
      "Epoch 14/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.4513 - accuracy: 0.4115 - val_loss: 2.5377 - val_accuracy: 0.4167\n",
      "Epoch 15/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.4217 - accuracy: 0.4175 - val_loss: 2.5211 - val_accuracy: 0.4145\n",
      "Epoch 16/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.3926 - accuracy: 0.4224 - val_loss: 2.4987 - val_accuracy: 0.4241\n",
      "Epoch 17/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.3652 - accuracy: 0.4274 - val_loss: 2.4798 - val_accuracy: 0.4305\n",
      "Epoch 18/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.3402 - accuracy: 0.4348 - val_loss: 2.4623 - val_accuracy: 0.4318\n",
      "Epoch 19/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.3141 - accuracy: 0.4402 - val_loss: 2.4440 - val_accuracy: 0.4356\n",
      "Epoch 20/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.2913 - accuracy: 0.4435 - val_loss: 2.4331 - val_accuracy: 0.4425\n",
      "Epoch 21/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.2714 - accuracy: 0.4503 - val_loss: 2.4225 - val_accuracy: 0.4415\n",
      "Epoch 22/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.2492 - accuracy: 0.4553 - val_loss: 2.4043 - val_accuracy: 0.4486\n",
      "Epoch 23/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.2291 - accuracy: 0.4569 - val_loss: 2.3943 - val_accuracy: 0.4518\n",
      "Epoch 24/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.2116 - accuracy: 0.4615 - val_loss: 2.3840 - val_accuracy: 0.4519\n",
      "Epoch 25/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1929 - accuracy: 0.4662 - val_loss: 2.3739 - val_accuracy: 0.4557\n",
      "Epoch 26/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1756 - accuracy: 0.4695 - val_loss: 2.3668 - val_accuracy: 0.4557\n",
      "Epoch 27/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1567 - accuracy: 0.4743 - val_loss: 2.3545 - val_accuracy: 0.4626\n",
      "Epoch 28/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1362 - accuracy: 0.4773 - val_loss: 2.3455 - val_accuracy: 0.4633\n",
      "Epoch 29/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1276 - accuracy: 0.4794 - val_loss: 2.3403 - val_accuracy: 0.4685\n",
      "Epoch 30/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.1073 - accuracy: 0.4841 - val_loss: 2.3316 - val_accuracy: 0.4684\n",
      "Epoch 31/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0966 - accuracy: 0.4849 - val_loss: 2.3214 - val_accuracy: 0.4740\n",
      "Epoch 32/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0806 - accuracy: 0.4866 - val_loss: 2.3147 - val_accuracy: 0.4728\n",
      "Epoch 33/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0696 - accuracy: 0.4916 - val_loss: 2.3137 - val_accuracy: 0.4757\n",
      "Epoch 34/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0571 - accuracy: 0.4943 - val_loss: 2.3099 - val_accuracy: 0.4743\n",
      "Epoch 35/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0450 - accuracy: 0.4955 - val_loss: 2.3068 - val_accuracy: 0.4786\n",
      "Epoch 36/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0332 - accuracy: 0.4977 - val_loss: 2.2958 - val_accuracy: 0.4787\n",
      "Epoch 37/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0219 - accuracy: 0.5008 - val_loss: 2.2938 - val_accuracy: 0.4816\n",
      "Epoch 38/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 2.0087 - accuracy: 0.5034 - val_loss: 2.2890 - val_accuracy: 0.4846\n",
      "Epoch 39/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9980 - accuracy: 0.5078 - val_loss: 2.2845 - val_accuracy: 0.4821\n",
      "Epoch 40/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9868 - accuracy: 0.5086 - val_loss: 2.2756 - val_accuracy: 0.4877\n",
      "Epoch 41/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9812 - accuracy: 0.5105 - val_loss: 2.2787 - val_accuracy: 0.4900\n",
      "Epoch 42/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9657 - accuracy: 0.5143 - val_loss: 2.2740 - val_accuracy: 0.4893\n",
      "Epoch 43/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9573 - accuracy: 0.5160 - val_loss: 2.2683 - val_accuracy: 0.4905\n",
      "Epoch 44/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9416 - accuracy: 0.5190 - val_loss: 2.2668 - val_accuracy: 0.4934\n",
      "Epoch 45/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9386 - accuracy: 0.5200 - val_loss: 2.2601 - val_accuracy: 0.4962\n",
      "Epoch 46/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9282 - accuracy: 0.5214 - val_loss: 2.2583 - val_accuracy: 0.4955\n",
      "Epoch 47/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9178 - accuracy: 0.5240 - val_loss: 2.2567 - val_accuracy: 0.4987\n",
      "Epoch 48/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9064 - accuracy: 0.5273 - val_loss: 2.2556 - val_accuracy: 0.5007\n",
      "Epoch 49/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.9011 - accuracy: 0.5274 - val_loss: 2.2569 - val_accuracy: 0.4968\n",
      "Epoch 50/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8909 - accuracy: 0.5286 - val_loss: 2.2490 - val_accuracy: 0.5009\n",
      "Epoch 51/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8871 - accuracy: 0.5315 - val_loss: 2.2446 - val_accuracy: 0.5054\n",
      "Epoch 52/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8770 - accuracy: 0.5351 - val_loss: 2.2492 - val_accuracy: 0.5047\n",
      "Epoch 53/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8698 - accuracy: 0.5344 - val_loss: 2.2423 - val_accuracy: 0.5059\n",
      "Epoch 54/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8639 - accuracy: 0.5366 - val_loss: 2.2438 - val_accuracy: 0.5066\n",
      "Epoch 55/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8556 - accuracy: 0.5387 - val_loss: 2.2400 - val_accuracy: 0.5052\n",
      "Epoch 56/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8440 - accuracy: 0.5387 - val_loss: 2.2393 - val_accuracy: 0.5070\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8404 - accuracy: 0.5416 - val_loss: 2.2398 - val_accuracy: 0.5097\n",
      "Epoch 58/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8313 - accuracy: 0.5430 - val_loss: 2.2355 - val_accuracy: 0.5098\n",
      "Epoch 59/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8215 - accuracy: 0.5440 - val_loss: 2.2368 - val_accuracy: 0.5087\n",
      "Epoch 60/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8199 - accuracy: 0.5458 - val_loss: 2.2363 - val_accuracy: 0.5106\n",
      "Epoch 61/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8137 - accuracy: 0.5461 - val_loss: 2.2380 - val_accuracy: 0.5086\n",
      "Epoch 62/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.8023 - accuracy: 0.5491 - val_loss: 2.2371 - val_accuracy: 0.5132\n",
      "Epoch 63/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7962 - accuracy: 0.5521 - val_loss: 2.2333 - val_accuracy: 0.5137\n",
      "Epoch 64/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7987 - accuracy: 0.5497 - val_loss: 2.2324 - val_accuracy: 0.5176\n",
      "Epoch 65/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7856 - accuracy: 0.5527 - val_loss: 2.2361 - val_accuracy: 0.5139\n",
      "Epoch 66/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7837 - accuracy: 0.5548 - val_loss: 2.2362 - val_accuracy: 0.5181\n",
      "Epoch 67/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7709 - accuracy: 0.5560 - val_loss: 2.2319 - val_accuracy: 0.5162\n",
      "Epoch 68/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7623 - accuracy: 0.5585 - val_loss: 2.2320 - val_accuracy: 0.5168\n",
      "Epoch 69/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7638 - accuracy: 0.5586 - val_loss: 2.2300 - val_accuracy: 0.5213\n",
      "Epoch 70/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7537 - accuracy: 0.5598 - val_loss: 2.2327 - val_accuracy: 0.5157\n",
      "Epoch 71/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7525 - accuracy: 0.5598 - val_loss: 2.2317 - val_accuracy: 0.5199\n",
      "Epoch 72/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7439 - accuracy: 0.5613 - val_loss: 2.2344 - val_accuracy: 0.5190\n",
      "Epoch 73/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7408 - accuracy: 0.5632 - val_loss: 2.2327 - val_accuracy: 0.5240\n",
      "Epoch 74/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7356 - accuracy: 0.5663 - val_loss: 2.2349 - val_accuracy: 0.5224\n",
      "Epoch 75/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7262 - accuracy: 0.5672 - val_loss: 2.2330 - val_accuracy: 0.5231\n",
      "Epoch 76/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7232 - accuracy: 0.5677 - val_loss: 2.2317 - val_accuracy: 0.5229\n",
      "Epoch 77/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7197 - accuracy: 0.5671 - val_loss: 2.2359 - val_accuracy: 0.5214\n",
      "Epoch 78/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7204 - accuracy: 0.5697 - val_loss: 2.2287 - val_accuracy: 0.5248\n",
      "Epoch 79/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7088 - accuracy: 0.5703 - val_loss: 2.2316 - val_accuracy: 0.5257\n",
      "Epoch 80/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.7049 - accuracy: 0.5701 - val_loss: 2.2393 - val_accuracy: 0.5246\n",
      "Epoch 81/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6986 - accuracy: 0.5713 - val_loss: 2.2368 - val_accuracy: 0.5268\n",
      "Epoch 82/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6935 - accuracy: 0.5727 - val_loss: 2.2314 - val_accuracy: 0.5281\n",
      "Epoch 83/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6897 - accuracy: 0.5740 - val_loss: 2.2371 - val_accuracy: 0.5282\n",
      "Epoch 84/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6872 - accuracy: 0.5775 - val_loss: 2.2326 - val_accuracy: 0.5293\n",
      "Epoch 85/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6821 - accuracy: 0.5776 - val_loss: 2.2338 - val_accuracy: 0.5294\n",
      "Epoch 86/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6768 - accuracy: 0.5781 - val_loss: 2.2334 - val_accuracy: 0.5281\n",
      "Epoch 87/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6685 - accuracy: 0.5810 - val_loss: 2.2389 - val_accuracy: 0.5271\n",
      "Epoch 88/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6675 - accuracy: 0.5791 - val_loss: 2.2369 - val_accuracy: 0.5304\n",
      "Epoch 89/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6648 - accuracy: 0.5802 - val_loss: 2.2414 - val_accuracy: 0.5288\n",
      "Epoch 90/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6595 - accuracy: 0.5821 - val_loss: 2.2402 - val_accuracy: 0.5322\n",
      "Epoch 91/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6565 - accuracy: 0.5830 - val_loss: 2.2432 - val_accuracy: 0.5291\n",
      "Epoch 92/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6496 - accuracy: 0.5839 - val_loss: 2.2412 - val_accuracy: 0.5314\n",
      "Epoch 93/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6537 - accuracy: 0.5827 - val_loss: 2.2430 - val_accuracy: 0.5328\n",
      "Epoch 94/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6414 - accuracy: 0.5837 - val_loss: 2.2434 - val_accuracy: 0.5342\n",
      "Epoch 95/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6385 - accuracy: 0.5861 - val_loss: 2.2398 - val_accuracy: 0.5308\n",
      "Epoch 96/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6334 - accuracy: 0.5874 - val_loss: 2.2481 - val_accuracy: 0.5323\n",
      "Epoch 97/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6276 - accuracy: 0.5886 - val_loss: 2.2405 - val_accuracy: 0.5353\n",
      "Epoch 98/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6305 - accuracy: 0.5869 - val_loss: 2.2416 - val_accuracy: 0.5349\n",
      "Epoch 99/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6246 - accuracy: 0.5896 - val_loss: 2.2475 - val_accuracy: 0.5339\n",
      "Epoch 100/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6168 - accuracy: 0.5887 - val_loss: 2.2498 - val_accuracy: 0.5345\n",
      "Epoch 101/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6130 - accuracy: 0.5934 - val_loss: 2.2499 - val_accuracy: 0.5350\n",
      "Epoch 102/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6148 - accuracy: 0.5901 - val_loss: 2.2540 - val_accuracy: 0.5359\n",
      "Epoch 103/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6072 - accuracy: 0.5939 - val_loss: 2.2539 - val_accuracy: 0.5362\n",
      "Epoch 104/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6048 - accuracy: 0.5943 - val_loss: 2.2487 - val_accuracy: 0.5354\n",
      "Epoch 105/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.6002 - accuracy: 0.5950 - val_loss: 2.2572 - val_accuracy: 0.5354\n",
      "Epoch 106/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5960 - accuracy: 0.5967 - val_loss: 2.2638 - val_accuracy: 0.5360\n",
      "Epoch 107/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5921 - accuracy: 0.5956 - val_loss: 2.2554 - val_accuracy: 0.5363\n",
      "Epoch 108/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5889 - accuracy: 0.5978 - val_loss: 2.2588 - val_accuracy: 0.5357\n",
      "Epoch 109/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5877 - accuracy: 0.5967 - val_loss: 2.2607 - val_accuracy: 0.5364\n",
      "Epoch 110/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5873 - accuracy: 0.5949 - val_loss: 2.2670 - val_accuracy: 0.5342\n",
      "Epoch 111/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5822 - accuracy: 0.5970 - val_loss: 2.2650 - val_accuracy: 0.5390\n",
      "Epoch 112/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5797 - accuracy: 0.5998 - val_loss: 2.2637 - val_accuracy: 0.5378\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5766 - accuracy: 0.6004 - val_loss: 2.2639 - val_accuracy: 0.5366\n",
      "Epoch 114/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5728 - accuracy: 0.6004 - val_loss: 2.2671 - val_accuracy: 0.5387\n",
      "Epoch 115/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5642 - accuracy: 0.6036 - val_loss: 2.2646 - val_accuracy: 0.5410\n",
      "Epoch 116/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5622 - accuracy: 0.6018 - val_loss: 2.2665 - val_accuracy: 0.5409\n",
      "Epoch 117/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5630 - accuracy: 0.6048 - val_loss: 2.2686 - val_accuracy: 0.5413\n",
      "Epoch 118/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5563 - accuracy: 0.6045 - val_loss: 2.2708 - val_accuracy: 0.5391\n",
      "Epoch 119/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5522 - accuracy: 0.6041 - val_loss: 2.2715 - val_accuracy: 0.5404\n",
      "Epoch 120/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5506 - accuracy: 0.6059 - val_loss: 2.2691 - val_accuracy: 0.5407\n",
      "Epoch 121/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5508 - accuracy: 0.6065 - val_loss: 2.2764 - val_accuracy: 0.5408\n",
      "Epoch 122/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5430 - accuracy: 0.6082 - val_loss: 2.2796 - val_accuracy: 0.5406\n",
      "Epoch 123/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5488 - accuracy: 0.6071 - val_loss: 2.2808 - val_accuracy: 0.5415\n",
      "Epoch 124/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5377 - accuracy: 0.6071 - val_loss: 2.2823 - val_accuracy: 0.5418\n",
      "Epoch 125/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5365 - accuracy: 0.6071 - val_loss: 2.2789 - val_accuracy: 0.5400\n",
      "Epoch 126/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5397 - accuracy: 0.6079 - val_loss: 2.2820 - val_accuracy: 0.5409\n",
      "Epoch 127/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5305 - accuracy: 0.6086 - val_loss: 2.2809 - val_accuracy: 0.5414\n",
      "Epoch 128/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5276 - accuracy: 0.6133 - val_loss: 2.2850 - val_accuracy: 0.5423\n",
      "Epoch 129/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5253 - accuracy: 0.6113 - val_loss: 2.2846 - val_accuracy: 0.5398\n",
      "Epoch 130/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5290 - accuracy: 0.6105 - val_loss: 2.2868 - val_accuracy: 0.5419\n",
      "Epoch 131/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5184 - accuracy: 0.6122 - val_loss: 2.2859 - val_accuracy: 0.5487\n",
      "Epoch 132/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5219 - accuracy: 0.6129 - val_loss: 2.2938 - val_accuracy: 0.5417\n",
      "Epoch 133/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5099 - accuracy: 0.6143 - val_loss: 2.2959 - val_accuracy: 0.5430\n",
      "Epoch 134/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5134 - accuracy: 0.6130 - val_loss: 2.2930 - val_accuracy: 0.5434\n",
      "Epoch 135/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5052 - accuracy: 0.6156 - val_loss: 2.2989 - val_accuracy: 0.5438\n",
      "Epoch 136/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5076 - accuracy: 0.6145 - val_loss: 2.2938 - val_accuracy: 0.5444\n",
      "Epoch 137/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5033 - accuracy: 0.6154 - val_loss: 2.3011 - val_accuracy: 0.5414\n",
      "Epoch 138/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4955 - accuracy: 0.6183 - val_loss: 2.3024 - val_accuracy: 0.5446\n",
      "Epoch 139/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.5007 - accuracy: 0.6186 - val_loss: 2.3101 - val_accuracy: 0.5429\n",
      "Epoch 140/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4922 - accuracy: 0.6182 - val_loss: 2.3100 - val_accuracy: 0.5465\n",
      "Epoch 141/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4878 - accuracy: 0.6175 - val_loss: 2.3100 - val_accuracy: 0.5466\n",
      "Epoch 142/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4908 - accuracy: 0.6175 - val_loss: 2.3124 - val_accuracy: 0.5440\n",
      "Epoch 143/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4964 - accuracy: 0.6182 - val_loss: 2.3078 - val_accuracy: 0.5468\n",
      "Epoch 144/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4904 - accuracy: 0.6190 - val_loss: 2.3185 - val_accuracy: 0.5455\n",
      "Epoch 145/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4830 - accuracy: 0.6213 - val_loss: 2.3136 - val_accuracy: 0.5481\n",
      "Epoch 146/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4808 - accuracy: 0.6220 - val_loss: 2.3143 - val_accuracy: 0.5448\n",
      "Epoch 147/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4828 - accuracy: 0.6222 - val_loss: 2.3161 - val_accuracy: 0.5485\n",
      "Epoch 148/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4744 - accuracy: 0.6243 - val_loss: 2.3187 - val_accuracy: 0.5450\n",
      "Epoch 149/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4776 - accuracy: 0.6225 - val_loss: 2.3208 - val_accuracy: 0.5484\n",
      "Epoch 150/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4712 - accuracy: 0.6228 - val_loss: 2.3234 - val_accuracy: 0.5457\n",
      "Epoch 151/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4715 - accuracy: 0.6249 - val_loss: 2.3237 - val_accuracy: 0.5475\n",
      "Epoch 152/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4660 - accuracy: 0.6236 - val_loss: 2.3323 - val_accuracy: 0.5493\n",
      "Epoch 153/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4663 - accuracy: 0.6269 - val_loss: 2.3324 - val_accuracy: 0.5445\n",
      "Epoch 154/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4556 - accuracy: 0.6272 - val_loss: 2.3314 - val_accuracy: 0.5497\n",
      "Epoch 155/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4609 - accuracy: 0.6244 - val_loss: 2.3337 - val_accuracy: 0.5502\n",
      "Epoch 156/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4614 - accuracy: 0.6247 - val_loss: 2.3318 - val_accuracy: 0.5470\n",
      "Epoch 157/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4593 - accuracy: 0.6270 - val_loss: 2.3405 - val_accuracy: 0.5476\n",
      "Epoch 158/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4529 - accuracy: 0.6272 - val_loss: 2.3401 - val_accuracy: 0.5460\n",
      "Epoch 159/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4478 - accuracy: 0.6309 - val_loss: 2.3377 - val_accuracy: 0.5465\n",
      "Epoch 160/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4536 - accuracy: 0.6292 - val_loss: 2.3428 - val_accuracy: 0.5470\n",
      "Epoch 161/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4458 - accuracy: 0.6278 - val_loss: 2.3466 - val_accuracy: 0.5488\n",
      "Epoch 162/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4419 - accuracy: 0.6296 - val_loss: 2.3495 - val_accuracy: 0.5476\n",
      "Epoch 163/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4459 - accuracy: 0.6296 - val_loss: 2.3493 - val_accuracy: 0.5471\n",
      "Epoch 164/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4426 - accuracy: 0.6277 - val_loss: 2.3530 - val_accuracy: 0.5475\n",
      "Epoch 165/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4395 - accuracy: 0.6302 - val_loss: 2.3542 - val_accuracy: 0.5486\n",
      "Epoch 166/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4395 - accuracy: 0.6312 - val_loss: 2.3578 - val_accuracy: 0.5476\n",
      "Epoch 167/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4332 - accuracy: 0.6340 - val_loss: 2.3562 - val_accuracy: 0.5459\n",
      "Epoch 168/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4347 - accuracy: 0.6318 - val_loss: 2.3520 - val_accuracy: 0.5515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4276 - accuracy: 0.6341 - val_loss: 2.3634 - val_accuracy: 0.5483\n",
      "Epoch 170/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4305 - accuracy: 0.6340 - val_loss: 2.3588 - val_accuracy: 0.5517\n",
      "Epoch 171/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4282 - accuracy: 0.6330 - val_loss: 2.3630 - val_accuracy: 0.5511\n",
      "Epoch 172/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4297 - accuracy: 0.6338 - val_loss: 2.3628 - val_accuracy: 0.5489\n",
      "Epoch 173/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4216 - accuracy: 0.6359 - val_loss: 2.3741 - val_accuracy: 0.5479\n",
      "Epoch 174/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4227 - accuracy: 0.6355 - val_loss: 2.3750 - val_accuracy: 0.5502\n",
      "Epoch 175/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4258 - accuracy: 0.6332 - val_loss: 2.3709 - val_accuracy: 0.5492\n",
      "Epoch 176/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4182 - accuracy: 0.6359 - val_loss: 2.3701 - val_accuracy: 0.5517\n",
      "Epoch 177/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4137 - accuracy: 0.6347 - val_loss: 2.3729 - val_accuracy: 0.5508\n",
      "Epoch 178/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4175 - accuracy: 0.6341 - val_loss: 2.3883 - val_accuracy: 0.5470\n",
      "Epoch 179/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4095 - accuracy: 0.6378 - val_loss: 2.3764 - val_accuracy: 0.5512\n",
      "Epoch 180/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4131 - accuracy: 0.6372 - val_loss: 2.3857 - val_accuracy: 0.5500\n",
      "Epoch 181/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4077 - accuracy: 0.6387 - val_loss: 2.3804 - val_accuracy: 0.5516\n",
      "Epoch 182/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4050 - accuracy: 0.6412 - val_loss: 2.3790 - val_accuracy: 0.5511\n",
      "Epoch 183/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4071 - accuracy: 0.6391 - val_loss: 2.3857 - val_accuracy: 0.5524\n",
      "Epoch 184/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.4006 - accuracy: 0.6381 - val_loss: 2.3850 - val_accuracy: 0.5518\n",
      "Epoch 185/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3920 - accuracy: 0.6409 - val_loss: 2.3847 - val_accuracy: 0.5541\n",
      "Epoch 186/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3915 - accuracy: 0.6420 - val_loss: 2.3952 - val_accuracy: 0.5515\n",
      "Epoch 187/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3987 - accuracy: 0.6387 - val_loss: 2.3979 - val_accuracy: 0.5519\n",
      "Epoch 188/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3980 - accuracy: 0.6393 - val_loss: 2.3948 - val_accuracy: 0.5539\n",
      "Epoch 189/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3945 - accuracy: 0.6401 - val_loss: 2.3958 - val_accuracy: 0.5538\n",
      "Epoch 190/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3823 - accuracy: 0.6435 - val_loss: 2.4003 - val_accuracy: 0.5515\n",
      "Epoch 191/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3846 - accuracy: 0.6425 - val_loss: 2.3969 - val_accuracy: 0.5527\n",
      "Epoch 192/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3841 - accuracy: 0.6419 - val_loss: 2.3993 - val_accuracy: 0.5534\n",
      "Epoch 193/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3856 - accuracy: 0.6418 - val_loss: 2.4061 - val_accuracy: 0.5518\n",
      "Epoch 194/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3842 - accuracy: 0.6436 - val_loss: 2.4098 - val_accuracy: 0.5521\n",
      "Epoch 195/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3823 - accuracy: 0.6452 - val_loss: 2.4047 - val_accuracy: 0.5527\n",
      "Epoch 196/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3798 - accuracy: 0.6450 - val_loss: 2.4090 - val_accuracy: 0.5529\n",
      "Epoch 197/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3740 - accuracy: 0.6471 - val_loss: 2.4155 - val_accuracy: 0.5542\n",
      "Epoch 198/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3750 - accuracy: 0.6447 - val_loss: 2.4117 - val_accuracy: 0.5552\n",
      "Epoch 199/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3733 - accuracy: 0.6473 - val_loss: 2.4138 - val_accuracy: 0.5544\n",
      "Epoch 200/200\n",
      "1905/1905 [==============================] - 4s 2ms/step - loss: 1.3789 - accuracy: 0.6436 - val_loss: 2.4107 - val_accuracy: 0.5546\n"
     ]
    }
   ],
   "source": [
    "labels_for_tasks = gen_task_classes(len(train_data), 7, shuffle=True)\n",
    "data, labels = [], []\n",
    "for i, classes in enumerate(labels_for_tasks):\n",
    "    d, l = get_data(train_data, classes, idx_to_class_name, return_size=1000, neg_samples=None)\n",
    "    data.append(d)\n",
    "    labels.append(l)#[i] * d.shape[0]\n",
    "data = np.vstack(data)\n",
    "labels = np.hstack(labels)\n",
    "idx = np.random.choice(data.shape[0], data.shape[0], replace=False)\n",
    "data = data[idx]\n",
    "labels = labels[idx]\n",
    "print(data.shape, labels.shape)\n",
    "model = fit_basic_model(data, labels, verbose=True, epochs=200)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
