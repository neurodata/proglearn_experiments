{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to try some experiments to assess the baseline performance of single task (city) performance on post COVID data. This gives us an indication on what to improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names():\n",
    "    df = pd.read_csv(\"Data_Processed_New/NYISO_mobility_annotated.csv\")\n",
    "    df_cols = np.array(df.columns)\n",
    "    \n",
    "    response_col_name = df_cols[1]\n",
    "    weather_col_names = df_cols[2:7]\n",
    "    timestamp_col_names = df_cols[12:43]\n",
    "    holiday_col_names = [df_cols[43]]\n",
    "    mobility_col_names = df_cols[44:53]\n",
    "    \n",
    "    return response_col_name, weather_col_names, timestamp_col_names, holiday_col_names, mobility_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_col_name, weather_col_names, timestamp_col_names, holiday_col_names, mobility_col_names = get_col_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The city_name field can take on \"Boston\", \"Chicago\", \"Dallas\", \"Houston\", \"Phil\", \"SA\", or \"Seattle\".\n",
    "def load_data(city_name, standardize = False, verbose = False, no_mobility = False):\n",
    "    df = np.genfromtxt('Data_Processed_New/City_Level/%s_mobility_all.csv' % city_name, delimiter=',')\n",
    "    \n",
    "    X_weather = df[:, 1:1+len(weather_col_names)].astype(float)\n",
    "    X_holiday = df[:, 1+len(holiday_col_names)+len(weather_col_names)+len(timestamp_col_names)].astype(float).reshape(-1, 1)\n",
    "    X_mobility = df[:, 1+len(holiday_col_names)+len(weather_col_names)+len(timestamp_col_names):].astype(float)    \n",
    "    if no_mobility:\n",
    "        X = np.hstack((X_weather, X_holiday))\n",
    "    else:\n",
    "        X = np.hstack((X_weather, X_holiday, X_mobility))\n",
    "    \n",
    "    if standardize: X = scale(X)\n",
    "    \n",
    "    timestamps = df[:, 1+len(weather_col_names):1+len(weather_col_names)+len(timestamp_col_names)].astype(int)\n",
    "    y = df[:, 0].astype(float)\n",
    "    \n",
    "    metadata = {\"city_name\" : city_name, \n",
    "                \"X_weather\" : X_weather, \n",
    "                \"X_holiday\": X_holiday,\n",
    "                \"X_mobility\" : X_mobility, \n",
    "                \"timestamps\" : timestamps}\n",
    "\n",
    "    if verbose:\n",
    "        print(city_name, \"Energy Consumption Data:\")\n",
    "        print(\"Sample size n =\", X.shape[0])\n",
    "        print(\"Number of weather features d_w =\", X_weather.shape[1])\n",
    "        print(\"Number of holiday features d_h =\", X_holiday.shape[1])\n",
    "        if not no_mobility:\n",
    "            print(\"Number of mobility features d_m =\", X_mobility.shape[1])\n",
    "        print(\"Total number features d =\", X.shape[1])\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Energy Consumption Data:\n",
      "Sample size n = 2184\n",
      "Number of weather features d_w = 5\n",
      "Number of holiday features d_h = 1\n",
      "Total number features d = 6\n"
     ]
    }
   ],
   "source": [
    "X, y, metadata = load_data(\"Boston\", verbose = True, no_mobility = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds past output values as inputs. 'p' is the number of lagged values to append.\n",
    "# 'ps' is an array of lags. 'scales' is an array which determines the intervals of those lags.\n",
    "def append_past_outputs(X, y, p_hour, p_day, p_week):\n",
    "    X, y = check_X_y(X, y)\n",
    "    if p_hour == 0 and p_day == 0 and p_week == 0:\n",
    "        return X, y\n",
    "    \n",
    "    hour_time = 1\n",
    "    day_time = int(hour_time * 24)\n",
    "    week_time = int(day_time * 7)\n",
    "    \n",
    "    ps = [int(p_hour), int(p_day), int(p_week)]\n",
    "    times = [hour_time, day_time, week_time]\n",
    "    burnin = int(max(np.array(ps) * np.array(times)))\n",
    "    \n",
    "    n = len(X)\n",
    "    n_new = n - burnin\n",
    "    X_new = X[burnin:]\n",
    "    y_new = y[burnin:]\n",
    "    \n",
    "    # Make numpy array of indices, and use it to index/map y.\n",
    "    # i is the training example, j is the lag.\n",
    "    for p in range(3):\n",
    "        if ps[p] != 0:\n",
    "            idx = np.fromfunction(lambda i, j: burnin + i - times[p]*(j + 1), (n_new, ps[p]), dtype = np.int32)\n",
    "            X_new = np.hstack((X_new, y[idx]))\n",
    "    \n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds past output values as inputs. 'p' is the number of lagged values to append.\n",
    "# 'ps' is an array of lags. 'scales' is an array which determines the intervals of those lags.\n",
    "def append_past_inputs(X, y, p_hour, p_day, p_week):\n",
    "    X, y = check_X_y(X, y)\n",
    "    if p_hour == 0 and p_day == 0 and p_week == 0:\n",
    "        return X, y\n",
    "    \n",
    "    hour_time = 1\n",
    "    day_time = int(hour_time * 24)\n",
    "    week_time = int(day_time * 7)\n",
    "    \n",
    "    ps = [int(p_hour), int(p_day), int(p_week)]\n",
    "    times = [hour_time, day_time, week_time]\n",
    "    burnin = int(max(np.array(ps) * np.array(times)))\n",
    "    \n",
    "    n = len(X)\n",
    "    n_new = n - burnin\n",
    "    X_new = X[burnin:]\n",
    "    y_new = y[burnin:]\n",
    "    \n",
    "    # add hourly features from up to p_hour lags ago.\n",
    "    if p_hour != 0:\n",
    "        d = X_new.shape[1]\n",
    "        X_hour = np.zeros((n_new, int(p_hour)*d))\n",
    "        for i in range(n_new):\n",
    "            for j in range(int(p_hour)):\n",
    "                for k in range(d):\n",
    "                    X_hour[i, j*d+k] = X[burnin + i - (j + 1), k]\n",
    "        X_new = np.hstack((X_new, X_hour))\n",
    "    \n",
    "    # Make numpy array of indices, and use it to index/map y.\n",
    "    # i is the training example, j is the lag.\n",
    "    for p in range(1, 3):\n",
    "        if ps[p] != 0:\n",
    "            idx = np.fromfunction(lambda i, j: burnin + i - times[p]*(j + 1), (n_new, ps[p]), dtype = np.int32)\n",
    "            X_new = np.hstack((X_new, y[idx]))\n",
    "    \n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected X size = (132, 11)\n",
      "Observed X size = (132, 11)\n",
      "Expected y size = (132,)\n",
      "Observed y size = (132,)\n",
      "Expected X[0] = [168. 1. 167. 1. 166. 1. 165. 1. 144. 120. 0.]\n",
      "Observed X[0] = [168.   1. 167.   1. 166.   1. 165.   1. 144. 120.   0.]\n"
     ]
    }
   ],
   "source": [
    "# Test append past values.\n",
    "n = 300\n",
    "d = 2\n",
    "\n",
    "X = np.hstack((np.arange(n).reshape((n,1)), np.ones((n, 1))))\n",
    "y = np.arange(n)\n",
    "\n",
    "p_hour = 3\n",
    "p_day = 2\n",
    "p_week = 1\n",
    "\n",
    "X, y = append_past_inputs(X, y, p_hour, p_day, p_week)\n",
    "\n",
    "print(\"Expected X size = (132, 11)\")\n",
    "print(\"Observed X size =\", X.shape)\n",
    "\n",
    "print(\"Expected y size = (132,)\")\n",
    "print(\"Observed y size =\", y.shape)\n",
    "\n",
    "print(\"Expected X[0] = [168. 1. 167. 1. 166. 1. 165. 1. 144. 120. 0.]\")\n",
    "print(\"Observed X[0] =\", X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the data are time series, this can only be done with one fold.\n",
    "def train_val_test_split(X, y):\n",
    "    X, y = check_X_y(X, y)\n",
    "    \n",
    "    n = len(y)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = (n - n_train) // 2\n",
    "    n_test = n - n_train - n_val\n",
    "    \n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "    X_test, y_test = X[n_train+n_val:], y[n_train+n_val:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected y_test = [45 46 47 48 49]\n",
      "Observed y_test = [45 46 47 48 49]\n"
     ]
    }
   ],
   "source": [
    "# Test append past values.\n",
    "n = 50\n",
    "d = 10\n",
    "X = np.zeros((n, d))\n",
    "y = np.arange(n)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)\n",
    "\n",
    "print(\"Expected y_test = [45 46 47 48 49]\")\n",
    "print(\"Observed y_test =\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate hyper parameters array\n",
    "def sample_hyperparams(num_settings):\n",
    "    hyperparams = np.zeros((num_settings, 4))\n",
    "    for i in range(num_settings):\n",
    "        \n",
    "        hyperparams[i, 0] = 10 ** np.random.uniform(-3, 1) # regularization param\n",
    "        # hyperparams[i, 1] = np.random.randint(9) # p_hour\n",
    "        hyperparams[i, 2] = np.random.randint(6) # p_day\n",
    "        hyperparams[i, 3] = np.random.randint(3) # p_day\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape of hyperparams = (100, 4)\n",
      "Observed shape of hyperparams = (100, 4)\n",
      "[6.98213488e-03 2.44352772e+00 4.83400453e+00 1.23135815e-01\n",
      " 1.54238555e-03 1.73344255e-03 3.97197165e-03 7.37929482e+00\n",
      " 1.39640627e-02]\n",
      "\n",
      " Plot should be discrete uniform over [0, 2], [0, 5], [0, 8], respectively\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of SAR lags')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_settings = 100\n",
    "hyperparams = sample_hyperparams(num_settings)\n",
    "\n",
    "print(\"Expected shape of hyperparams = (100, 4)\")\n",
    "print(\"Observed shape of hyperparams =\", hyperparams.shape)\n",
    "\n",
    "# Check lambdas for log scale\n",
    "print(hyperparams[1:10, 0])\n",
    "\n",
    "# Check for uniformity.\n",
    "print(\"\\n Plot should be discrete uniform over [0, 2], [0, 5], [0, 8], respectively\")\n",
    "scales = [\"p_hour\", \"p_day\", \"p_week\"]\n",
    "for i in range(3):\n",
    "    plt.hist(hyperparams[:, i], label = scales[i], align = 'left')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of SAR lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_name, X_train, y_train, lambda_):\n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestRegressor(n_estimators=300)\n",
    "        return model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model = Ridge(alpha=lambda_)\n",
    "        return model.fit(X_train, y_train)\n",
    "\n",
    "# relative error\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_pred = check_array(y_pred, ensure_2d=False)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def compute_error(model, X_val, y_val):\n",
    "    X_val, y_val = check_X_y(X_val, y_val)\n",
    "    y_pred = model.predict(X_val)\n",
    "    return mean_absolute_percentage_error(y_val, y_pred)\n",
    "\n",
    "def compute_cheat_test_error(model_name, X, y, hyperparams):\n",
    "    lambda_, p_hour, p_day, p_week = hyperparams[\"lambda\"], hyperparams[\"p_hour\"], hyperparams[\"p_day\"], hyperparams[\"p_week\"] \n",
    "    \n",
    "    X_aug, y_aug = append_past_outputs(X, y, p_hour, p_day, p_week)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_aug, y_aug)\n",
    "    \n",
    "    X_full = np.vstack((X_train, X_val))\n",
    "    y_full = np.concatenate((y_train, y_val))\n",
    "    model = fit_model(model, X_full, y_full, lambda_)\n",
    "    \n",
    "    return compute_error(model, X_test, y_test), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected relative error = 1.0\n",
      "Observed relative error = 1.0\n",
      "Expected relative error is very small.\n",
      "Observed relative error is 1.1724925517371533e-15\n"
     ]
    }
   ],
   "source": [
    "# Test rel_Error\n",
    "y_pred = np.array([0, 1, 2, 3])\n",
    "y_true = np.array([1, 1, 1, 1])\n",
    "rel_err = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(\"Expected relative error = 1.0\")\n",
    "print(\"Observed relative error =\", rel_err)\n",
    "\n",
    "# \"Test\" linear\n",
    "X = np.random.randn(10, 3)\n",
    "y = X[:, 0]\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)\n",
    "model = fit_model('ridge', X_train, y_train, 0.0)\n",
    "print(\"Expected relative error is very small.\")\n",
    "print(\"Observed relative error is\", compute_error(model, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing test error is a little bit tricky, as we will not have access to all of the features on the onset. Instead, we will need to predict them 24 hours in advance, and use the predictions as features for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_error(model_name, X, y, hyperparams):\n",
    "    lambda_, p_hour, p_day, p_week = hyperparams[\"lambda\"], hyperparams[\"p_hour\"], hyperparams[\"p_day\"], hyperparams[\"p_week\"] \n",
    "    \n",
    "    d = X.shape[1]\n",
    "    X_aug, y_aug = append_past_outputs(X, y, p_hour, p_day, p_week)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_aug, y_aug)\n",
    "    \n",
    "    X_retrain = np.vstack((X_train, X_val))\n",
    "    y_retrain = np.concatenate((y_train, y_val))\n",
    "    model = fit_model(model_name, X_retrain, y_retrain, lambda_)\n",
    "                \n",
    "    return backtest_error(model, X_aug, y_test, p_hour, d), model\n",
    "\n",
    "def backtest_error(model, X_aug, y_test, p_hour, d):\n",
    "    # Compute error.\n",
    "    n_aug, d_aug = X_aug.shape\n",
    "    n_test = len(y_test)\n",
    "    n_retrain = n_aug - n_test\n",
    "    y_pred = np.zeros(n_test)\n",
    "    X = np.copy(X_aug) \n",
    "    for i in range(n_test):\n",
    "        # Index in time.\n",
    "        t = i + n_retrain\n",
    "        y_pred[i] = model.predict(X[t].reshape(1, -1))\n",
    "        \n",
    "        # Populate predictions.\n",
    "        for p in range(int(p_hour)):\n",
    "            if t + p < n_aug and d + p < d_aug:\n",
    "                X[t + p, d + p] = y_pred[i]\n",
    "    \n",
    "    return mean_absolute_percentage_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected error = 6.333333333333333\n",
      "Observed error = 6.333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Test backtest\n",
    "class TestModel():\n",
    "    def __init__(self, p_hour):\n",
    "        self.p_hour = p_hour\n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        return X[:, self.p_hour] + X[:, self.p_hour + 1]\n",
    "\n",
    "p_hour = 2\n",
    "n = 5\n",
    "d = 2\n",
    "n_test = 3\n",
    "\n",
    "X_aug = np.hstack((np.zeros((n, d)), np.fromfunction(lambda i, j: i, (n, p_hour))))\n",
    "y_test = np.ones(n_test)\n",
    "model = TestModel(p_hour)\n",
    "\n",
    "err = backtest_error(model, X_aug, y_test, p_hour, d)\n",
    "\n",
    "# y_0 = 1; yhat_0 = X_2[2] + X_3[3] = 2 + 2 = 4. error = 3 / 1 = 3\n",
    "# y_1 = 1; yhat_1 = X_3[2] + X_3[3] = yhat_0 + X_3[3] = 4 + 3 = 7. error = 6 / 1 = 6\n",
    "# y_2 = 1; yhat_2 = X_4[2] + X_4[3] = yhat_1 + X_4[3] = 7 + 4 = 11. error = 10 / 1 = 10\n",
    "# avg = 19/3\n",
    "\n",
    "print(\"Expected error =\", 19/3)\n",
    "print(\"Observed error =\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(city_name, num_settings, model_name, no_mobility = False):\n",
    "    X, y, metadata = load_data(city_name, standardize = True, verbose = False, no_mobility = no_mobility)\n",
    "    hyperparams = sample_hyperparams(num_settings)\n",
    "\n",
    "    def worker(row):\n",
    "        # Collect hyperparameters.\n",
    "        lambda_, p_hour, p_day, p_week = row\n",
    "        \n",
    "        # Build features.\n",
    "        X_aug, y_aug = append_past_outputs(X, y, p_hour, p_day, p_week)\n",
    "        \n",
    "        # Latest timesteps are test, second latest are validation, rest is train.\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_aug, y_aug)\n",
    "        \n",
    "        # Compute validation error.\n",
    "        model = fit_model(model_name, X_train, y_train, lambda_)\n",
    "        rel_err = compute_error(model, X_val, y_val)\n",
    "\n",
    "        return rel_err\n",
    "\n",
    "    # non parallel version\n",
    "#     rel_errs = np.zeros(num_settings)\n",
    "#     for s in range(num_settings):\n",
    "#         rel_errs[s] = worker(hyperparams[s])\n",
    "        \n",
    "    rel_errs = np.array(Parallel(n_jobs=-2)(delayed(worker)(row) for row in hyperparams))\n",
    "    \n",
    "    argmin = np.argmin(rel_errs)\n",
    "    best_val_error = rel_errs[argmin]\n",
    "    best_hyperparams = {\n",
    "        \"lambda\" : hyperparams[argmin, 0],\n",
    "        \"p_hour\" : hyperparams[argmin, 1],\n",
    "        \"p_day\" : hyperparams[argmin, 2],\n",
    "        \"p_week\" : hyperparams[argmin, 3]\n",
    "    }\n",
    "    \n",
    "    print(\"City:\", city_name)\n",
    "    if no_mobility:\n",
    "        print(\"No mobility features used.\")\n",
    "    print(\"Best relative validation error:\", best_val_error)\n",
    "    print(\"Best hyperparmeters:\")\n",
    "    print(best_hyperparams)\n",
    "    print(\"model:\", model_name)\n",
    "    \n",
    "    # Compute test loss error.\n",
    "    test_error, model = compute_test_error(model_name, X, y, best_hyperparams)\n",
    "    print(\"Test error: \", test_error)\n",
    "    \n",
    "    result = {\n",
    "        'model' : model,\n",
    "        'best_val_error' : best_val_error,\n",
    "        'best_hyperparams' : best_hyperparams,\n",
    "        'test_error' : test_error\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Seattle\n",
      "No mobility features used.\n",
      "Best relative validation error: 0.03862828435955593\n",
      "Best hyperparmeters:\n",
      "{'lambda': 0.0015532228836670863, 'p_hour': 0.0, 'p_day': 5.0, 'p_week': 0.0}\n",
      "model: ridge\n",
      "Test error:  0.044520386310836275\n"
     ]
    }
   ],
   "source": [
    "city_name = \"Seattle\"\n",
    "num_settings = 100\n",
    "model_name = 'ridge'\n",
    "no_mobility = True\n",
    "\n",
    "result = run_experiment(city_name, num_settings, model_name, no_mobility = no_mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Boston\n",
      "No mobility features used.\n",
      "Best relative validation error: 0.03289964629910225\n",
      "Best hyperparmeters:\n",
      "{'lambda': 1.0485716889386631, 'p_hour': 0.0, 'p_day': 3.0, 'p_week': 2.0}\n",
      "model: ridge\n",
      "Test error:  0.04465882536550039\n"
     ]
    }
   ],
   "source": [
    "city_name = \"Boston\"\n",
    "num_settings = 100\n",
    "model_name = 'ridge'\n",
    "no_mobility = True\n",
    "\n",
    "result = run_experiment(city_name, num_settings, model_name, no_mobility = no_mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: SA\n",
      "No mobility features used.\n",
      "Best relative validation error: 0.06025330553247977\n",
      "Best hyperparmeters:\n",
      "{'lambda': 0.006702660078940776, 'p_hour': 0.0, 'p_day': 4.0, 'p_week': 1.0}\n",
      "model: ridge\n",
      "Test error:  0.0999791822785781\n"
     ]
    }
   ],
   "source": [
    "city_name = \"SA\"\n",
    "num_settings = 40\n",
    "model_name = 'ridge'\n",
    "no_mobility = True\n",
    "\n",
    "result = run_experiment(city_name, num_settings, model_name, no_mobility = no_mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Houston\n",
      "Best relative validation error: 0.042110193605167844\n",
      "Best hyperparmeters:\n",
      "{'lambda': 0.011697476571696886, 'p_hour': 0.0, 'p_day': 1.0, 'p_week': 0.0}\n",
      "model: ridge\n",
      "Test error:  0.10953733875783513\n"
     ]
    }
   ],
   "source": [
    "city_name = \"Houston\"\n",
    "num_settings = 40\n",
    "model_name = 'ridge'\n",
    "no_mobility = False\n",
    "\n",
    "result = run_experiment(city_name, num_settings, model_name, no_mobility = no_mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Chicago\n",
      "No mobility features used.\n",
      "Best relative validation error: 0.028571289272884545\n",
      "Best hyperparmeters:\n",
      "{'lambda': 0.06339070472897894, 'p_hour': 0.0, 'p_day': 4.0, 'p_week': 1.0}\n",
      "model: ridge\n",
      "Test error:  0.041161566939127425\n"
     ]
    }
   ],
   "source": [
    "city_name = \"Chicago\"\n",
    "num_settings = 40\n",
    "model_name = 'ridge'\n",
    "no_mobility = True\n",
    "\n",
    "result = run_experiment(city_name, num_settings, model_name, no_mobility = no_mobility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
